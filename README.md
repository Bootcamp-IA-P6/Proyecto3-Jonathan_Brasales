# ğŸ•·ï¸ Web Scraper - Django, Selenium & Docker

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)
![Django](https://img.shields.io/badge/Django-5.0-green?logo=django&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)
![Selenium](https://img.shields.io/badge/Selenium-4.0-43B02A?logo=selenium&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-Database-003B57?logo=sqlite&logoColor=white)

> **Proyecto de web scraping automatizado con Django, Selenium y Docker.** Extrae informaciÃ³n de sitios web, la almacena en base de datos y mantiene los datos actualizados mediante tareas programadas.

---

## ğŸ¯ Acerca del Proyecto

Este proyecto surge como respuesta a un reto tÃ©cnico planteado para analizar el mercado tecnolÃ³gico mediante tÃ©cnicas de web scraping. El objetivo es validar quÃ© informaciÃ³n se puede extraer de sitios web y demostrar capacidades de automatizaciÃ³n, persistencia de datos y escalabilidad.

### Â¿QuÃ© hace este scraper?

Actualmente extrae el **"Recurso del dÃ­a"** de Wikipedia como prueba de concepto, demostrando que la arquitectura es extensible a otros sitios web como portales de empleo, marketplaces, o fuentes de datos pÃºblicas.

---
## ğŸ“‚ Estructura del Proyecto

```
webscraper/
â”œâ”€â”€ scraper/                      # AplicaciÃ³n principal de scraping
â”‚   â”œâ”€â”€ management/
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”‚       â””â”€â”€ scraper_wiki.py   # Comando personalizado Django
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ scrape.py             # LÃ³gica del scraper (POO)
â”‚   â”œâ”€â”€ models.py                 # Modelo de datos
â”‚   â”œâ”€â”€ tests.py                  # Tests unitarios
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ webscraper_project/           # ConfiguraciÃ³n del proyecto Django
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ Dockerfile                    # DefiniciÃ³n del contenedor
â”œâ”€â”€ docker-compose.yml            # OrquestaciÃ³n de servicios
â”œâ”€â”€ cronfile                      # ConfiguraciÃ³n de tareas programadas
â”œâ”€â”€ requirements.txt              # Dependencias Python
â”œâ”€â”€ manage.py                     # CLI de Django
â”œâ”€â”€ db.sqlite3                    # Base de datos (generada)
â””â”€â”€ README.md
```

**Componentes clave:**
- **`scrape.py`**: Contiene la clase del scraper con toda la lÃ³gica de extracciÃ³n
- **`scraper_wiki.py`**: Comando Django que orquesta la ejecuciÃ³n del scraper
- **`models.py`**: Define el esquema de la base de datos
- **`cronfile`**: Programa la ejecuciÃ³n automÃ¡tica del scraper

---

## âœ¨ CaracterÃ­sticas Principales

- **ğŸ¤– Scraping automatizado** con Selenium en modo headless
- **ğŸ’¾ Persistencia de datos** en base de datos SQLite con Django ORM
- **â° EjecuciÃ³n programada** mediante Cron Jobs (cada 5 minutos en pruebas)
- **ğŸ³ Completamente dockerizado** para facilitar despliegue y portabilidad
- **ğŸ“ Sistema de logs** para trazabilidad y debugging
- **ğŸ”’ ValidaciÃ³n de duplicados** para mantener la integridad de datos
- **ğŸ§ª Tests unitarios** para garantizar calidad del cÃ³digo
- **ğŸ—ï¸ Arquitectura modular** basada en POO y buenas prÃ¡cticas

---

## ğŸ› ï¸ TecnologÃ­as

**Backend & Framework:**
- Python 3.11
- Django 5.0
- Django ORM

**Web Scraping:**
- Selenium 4.0
- Firefox + GeckoDriver (headless)

**Base de Datos:**
- SQLite (fase inicial, migrable a PostgreSQL/MySQL)

**AutomatizaciÃ³n & Deployment:**
- Docker & Docker Compose
- Cron (dentro del contenedor)

**Control de Versiones:**
- Git / GitHub
- GestiÃ³n del proyecto: GitHub Projects

---

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- Docker y Docker Compose instalados
- Git

### InstalaciÃ³n y EjecuciÃ³n

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/Bootcamp-IA-P6/Proyecto3-Jonathan_Brasales.git
   cd webscraper
   ```

2. **Construir y levantar el contenedor**
   ```bash
   docker compose up --build -d
   ```

3. **Verificar que el contenedor estÃ¡ corriendo**
   ```bash
   docker ps
   ```
   
   DeberÃ­as ver un contenedor llamado `webscraper-server-1` en estado `Up`.

4. **El scraper se ejecuta automÃ¡ticamente cada 5 minutos**. Para verificar su funcionamiento inmediatamente, podemos usar:
   ```bash
   docker exec -it webscraper-server-1 python manage.py scraper_wiki
   ```

### Detener el Proyecto

```bash
docker compose down
```

Para detener sin eliminar datos:
```bash
docker compose stop
```

---

## ğŸ’» Uso y Comandos

### Ejecutar el Scraper Manualmente

```bash
docker exec -it webscraper-server-1 python manage.py scraper_wiki
```

### Ver logs del Cron

```bash
# OpciÃ³n 1: Desde fuera del contenedor
docker exec -it webscraper-server-1 cat /var/log/cron.log

# OpciÃ³n 2: Entrando al contenedor
docker exec -it webscraper-server-1 bash
cat /var/log/cron.log
```

---

## ğŸ—„ï¸ Base de Datos

Los datos extraÃ­dos se almacenan en **SQLite** con la siguiente estructura:

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id` | Integer | ID Ãºnico (Primary Key) |
| `title` | String | TÃ­tulo del recurso |
| `description` | Text | DescripciÃ³n completa |
| `image_url` | URL | URL de la imagen asociada |
| `source` | String | Fuente del dato (ej: Wikipedia) |
| `created_at` | DateTime | Fecha y hora de extracciÃ³n |

### Consultar la Base de Datos

```bash
# Acceder al contenedor
docker exec -it webscraper-server-1 bash

# Instalar SQLite (si no estÃ¡ instalado)
apt-get update && apt-get install -y sqlite3

# Abrir la base de datos
sqlite3 db.sqlite3

# Comandos Ãºtiles dentro de SQLite
.tables                           # Ver tablas
SELECT * FROM scraper_scrapeddata; # Consultar datos
.exit                             # Salir
```

---

## ğŸ§ª Testing

### Ejecutar Tests

```bash
docker exec -it webscraper-server-1 python manage.py test
```

### Cobertura de Tests

Los tests verifican:
- âœ… InserciÃ³n correcta de datos en la base de datos
- âœ… ValidaciÃ³n de duplicados
- âœ… EjecuciÃ³n del comando de scraping sin errores

Django crea una base de datos temporal durante los tests, por lo que no afecta los datos reales.

---

## â±ï¸ AutomatizaciÃ³n

El scraper se ejecuta automÃ¡ticamente mediante **Cron Jobs** configurados dentro del contenedor Docker.

### ConfiguraciÃ³n Actual

```cron
*/5 * * * * /usr/local/bin/python /app/manage.py scraper_wiki >> /var/log/cron.log 2>&1
```

**TraducciÃ³n:** Cada 5 minutos ejecuta el scraper y registra la salida en `/var/log/cron.log`.

### Modificar la Frecuencia

Edita el archivo `cronfile` antes de construir el contenedor:

```cron
# Cada hora
0 * * * * /usr/local/bin/python /app/manage.py scraper_wiki >> /var/log/cron.log 2>&1

# Cada dÃ­a a las 2 AM
0 2 * * * /usr/local/bin/python /app/manage.py scraper_wiki >> /var/log/cron.log 2>&1

# Cada lunes a las 9 AM
0 9 * * 1 /usr/local/bin/python /app/manage.py scraper_wiki >> /var/log/cron.log 2>&1
```

DespuÃ©s de modificar, reconstruye el contenedor:
```bash
docker compose up --build -d
```

### Verificar que Cron estÃ¡ Activo

```bash
docker exec -it webscraper-server-1 bash
ps aux | grep cron
```

---

## ğŸš€ PrÃ³ximos Pasos

- [ ] **Frontend interactivo** para visualizar datos en tiempo real
  - TecnologÃ­as candidatas: Django Templates, Chart.js, o React
- [ ] **Despliegue pÃºblico** en servidor accesible
  - Opciones: AWS, DigitalOcean, Heroku, Railway
- [ ] **IntegraciÃ³n con mÃºltiples sitios web** (portales de empleo, marketplaces)
- [ ] **MigraciÃ³n a PostgreSQL** para mayor escalabilidad
- [ ] **API REST** para consumir los datos desde aplicaciones externas
- [ ] **Sistema de notificaciones** cuando se detecten cambios relevantes

---

## ğŸ¤ Contribuciones

Aunque este es un proyecto individual, las contribuciones son bienvenidas. Si encuentras un bug o tienes una sugerencia:

1. **Abre un Issue** describiendo el problema o la mejora
2. **Fork** el repositorio
3. **Crea una rama** para tu feature (`git checkout -b feature/nueva-funcionalidad`)
4. **Commit** tus cambios (`git commit -m 'AÃ±adir nueva funcionalidad'`)
5. **Push** a la rama (`git push origin feature/nueva-funcionalidad`)
6. **Abre un Pull Request**

### Ãreas donde Puedes Contribuir

- ğŸ§ª Ampliar la suite de tests
- ğŸ¨ Desarrollar el frontend
- ğŸ“Š Crear visualizaciones de datos
- ğŸŒ AÃ±adir scrapers para otros sitios web
- ğŸ“– Mejorar la documentaciÃ³n
- ğŸ› Reportar y corregir bugs

---

## ğŸ“„ Licencia

Este proyecto se desarrolla con fines educativos como parte de un reto tÃ©cnico de web scraping y automatizaciÃ³n.

**Consideraciones Ã©ticas:**
- âœ… Se respeta el archivo `robots.txt` de los sitios web
- âœ… El scraping se realiza a intervalos razonables para no sobrecargar servidores
- âœ… Los datos se utilizan exclusivamente con fines de aprendizaje y anÃ¡lisis tÃ©cnico
- âœ… No se distribuyen datos sensibles o con copyright

---

## ğŸ‘¤ Autor

**Desarrollador:** Jonathan Brasales
**Proyecto:** Reto tÃ©cnico de Web Scraping
**Contacto:** 
- ğŸ’¼ LinkedIn: [jbrasales](https://www.linkedin.com/in/jbrasales/)
- ğŸ™ GitHub: [@tu-usuario](https://github.com/tu-usuario)

**Tablero del Proyecto:** [GitHub Projects](https://github.com/orgs/Bootcamp-IA-P6/projects/12)

---

## ğŸ™ Agradecimientos

- **FactorÃ­a F5** por los recursos y materiales de referencia
- Comunidad de **Django** y **Selenium** por la documentaciÃ³n
- **XYZ Corp** por plantear el reto tÃ©cnico

---

<div align="center">
  
**â­ Si este proyecto te resulta Ãºtil, considera darle una estrella â­**

Hecho con â¤ï¸ usando Python, Django y Docker

</div>
